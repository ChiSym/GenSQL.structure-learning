= Automated Modeling
Ulli Schaechtle <u.schaechtle@gmail.com>

== Overview

This guide will help you use the IQL auto-modeling project to build your own
models on tabular data. Once built, these models can be queried using a variety
of interfaces. This guide starts with building a model from an example dataset.
It then shows you how to build a model from a dataset you supply. And latter
sections go into more detail about different backends for auto-modeling and a number of other considerations.

== Installation

=== Dependencies

We rely on the Devenv package manager. Please find instructions to install it https://devenv.sh/getting-started/[here]. Devenv takes care of all our dependencies; currently confirmed to be working on Ubuntu and Mac.

Further, we require docker to be installed and to be runnable without `sudo`.
Installation instructions for Mac or Ubuntu are
https://docs.docker.com/engine/install/[here].

Then,

* if on linux, run postinstall script: https://docs.docker.com/engine/install/linux-postinstall/
* login using credentials sufficient for probcomp’s Docker org:
 ** if on linux run `docker login`
 ** if on mac, open the Docker app GUI and log in with credentials
* Pull the Loom Docker image:
 ** Confirm you have access to view https://hub.docker.com/r/probcomp/inferenceql.loom
 ** Run `docker pull probcomp/inferenceql.loom`

=== Download Auto-modeling

Install from git
[source,bash]
----
git clone git@github.com:OpenIQL/inferenceql.auto-modeling.git InferenceQL
----

=== Start the devenv shell

To install modeling capabilities, go to the auto-modeling directory:
[source,bash]
----
cd InferenceQL
----
and enter a development shell that includes all the dependencies by running:
[source,bash]
----
devenv shell
----
WARNING: on macOS, this step can take up to an hour. The reason is that certain dependencies get compiled from scratch. It will only be slow once. The second time you run the `devenv shell` command, it will succeed instantaneously.


== Building your first model

=== Add a dataset

Next, we will add the dataset we want to model.

We are going to be working with a dataset from a citizen science project on
satellites:

[source,bash]
----
cp test/satellites/data.csv data/data.csv
----


=== Run the auto-modeling pipeline

IQL auto-modeling is built around DVC (https://dvc.org/), a project that helps with version control of machine learning pipelines.

The steps for building a model are encoded as dvc pipelines (see `dvc.yaml`) All we have to do is run the pipeline and dvc takes care of the rest.

From the root auto-modeling directory (`InferenceQL`), we can run the following command to get a list of the pipeline stages and which files they will produce giving us an overview of the work that will be done to build the model.
[source,bash]
----
dvc stage list
----

Now to run all of the pipelines executing
[source,bash]
----
dvc repro
----
Run `dvc repro --help` to see different options of running the pipeline.


== Quality control plots

Before we use our model, how can we know it is any good?

Auto-modeling produces two quality control apps for comparing simulated data produced by the model and the original observed data.

You can find them in `qc/app` folder.

`qc-dashboard.html` shows comparisons between all combinations of columns, while `qc-splom.html` shows a scatter plot matrix of just numerical columns -- the latter is not so interesting here as none of the numerical columns are inferred automatically (though there are several numerical columns in the data, which one can add manually as in `test/satellites/params.yaml`).

These apps are fully contained in their respective html files, so they can be easily moved around or shared.

The premise of the qc-plots is following: if our model has truly learned the multi-variate distribution of the data then the marginal and pairwise-marginal distributions of simulations from the model should match those of the observed data. These qc-plots allow one to visually confirm this.

NOTE: The plots support a number of features including panning, zooming, cross-selection, and details on hover. There are also other options in the options menu. You can find more details in the QC section of this guide.

== Querying the model
There are different ways to query automatically built models from Python, Clojure and JavaScript.
Checkout the https://github.com/InferenceQL/inferenceql.query[inferenceQL.query repository] about how to query models and data.

== Configuring the pipeline

=== Data specifications/settings

There are a variety of settings in the `params.yaml` file in the auto-modeling root dir. The following might need to be changed depending on your dataset.

==== schema

Auto-modeling tries to guess the statistical data types in your CSV. You can see
which statistical types are guessed by running `dvc repro -f guess-schema` and then opening `data/schema.edn`.

If you want to manually set the data types for one or more columns you can do that in `schema` section in `params.yaml`.

TIP: If a statistical type cannot be guessed with confidence, auto-modeling chooses to ignore this column. You can add `default-stat-type: numerical` or `default-stat-type: nominal` if you prefer to define the stattypes of all columns that can't be guessed. This can save you some time when a lot of columns are ignored.

=== nullify

This setting allows you to specify which string values will be considered as
null values in your CSV. The system will treat strings as categories in a
categorical variable -- if you have, for example the string `NaN` encoding
missing data, you have to let the auto-modeling system know.

=== Inference-related settings

There are a number of settings in `params.yaml` file that allow you to control the inference process. See the section below on the CGPM backend for more details on these settings.

=== QC options

See the comments in the `qc` section of the `params.yaml` file for details on the various settings available for QC plots.


== Model-building backends

IQL Auto-modeling supports a number of model-building backends. The previous sections on model building used the default CGPM backend. We will provide some more background on the CGPM backend here and also provide information on using alternatives.

=== Switching between backends

Each backend is encoded as a `yaml` file. When `dvc repro -f` is run, the yaml file for backend currently named `dvc.yaml` is run. To switch to a different backend, rename `dvc.yaml` to any temporary name. And rename the yaml file for the backend to you want to use to `dvc.yaml`.

=== CGPM

==== Key points
* Default backend
* Written in Python
* Robust
* DVC yaml filename: `dvc.yaml`

==== Settings
The following settings in `params.yaml` allow you to control the inferece process using the default backend, CGPM.

- `sample_count` — This lets you set the number of CrossCat models to learn, which together will comprise the ensemble.
- `cgpm > minutes` — The amount of time (minutes) to spend on inference. Use this setting or `cgpm > iterations` but not both.
- `cgpm > iterations` — The number CGPM interations to spend on inference. Use this setting or `cgpm > minutes` but not both.

==== Outputs

The key artifacts produced are as follows.

===== Individual CrossCat models

In `data/xcat/`, you can find multiple CrossCat models. Each one is a `.edn` file named `sample.0.edn`, `sample.1.edn`, etc. Any one of these individual CrossCat models can be used in an Observable notebook or in the IQL Viz spreadsheet app.

===== Ensemble of CrossCat models

`data/sppl/merged.json` is a sum-product network representation of all of the individual CrossCat models merged together forming an ensemble. This file can be used by IQL Query to start an IQL query server. The query server can then respond to sum-product queries from both an Observable notebook and the IQL Viz spreadsheet app. This is covered in a latter section.

=== Loom and CGPM

==== Key points
* Loom used to learn structure
* CGPM used to learn hyperparameters
* Loom is written in C with Python bindings
* Robust
* DVC yaml filename: `dvc-loom.yaml`

==== Setup
TODO: notes on getting the Docker image.

==== Settings
All the settings in `params.yaml` that apply to the CGPM backend also apply to the LOOM + CGPM backend. In addition, there are the following.

- `loom > extra_passes` — The number of extra inference passes to perform when learning structure.

==== Outputs
The outputs produced are the same as those produced by the CGPM backend. Please see the ouputs section for that backend.

==== Running additional inference

Sometimes, a user may wish to run additional CGPM inference without losing progress previously made. In order to run additional inference, users can run

[source,bash]
----
bin/update-inference
----

This will keep the previous result and spend another _n_ minutes with inference, where _n_ is specified in the `params.yaml` under `cgpm > minutes`.

==== Set qualitative dependence constraints

We can tell CrossCat to consider sets' of columns to be dependent or independent to improve modeling results.

This can be set in the `params.yaml` file. For dependence, edit the `cgpm` section, for example:

[source,yaml]
----
cgpm:
  minutes: 1
  dependence:
    foo:
      -bar
    baz:
      - quagga
----

While the CrossCat implementation in CGPM takes dependence constraints, doing so throws a not-implemented-error. Hence, we apply a workaround:
    - Supply a map from a target column name to a list of column names.
    - Each column in said list gets moved to the target column's view.

Similarly, we can ensure independence:

[source,yaml]
----
cgpm:
  minutes: 1
  independence:
    foo:
      - bar
      - baz
    quagga:
      - foo
----

Independence is un-directional. For two columns foo and bar, setting `foo: - bar` and `bar: - foo` has the identical effect.

=== Clojurecat

==== Key points
* Written in Clojure
* Usable from both the JVM and the browser (JS environments)
* Fewest requirements
* Experimental (there are know issues)
* DVC yaml filename: `dvc-clojurecat.yaml`

==== Settings
- `clojurecat > iterations` — This setting controls the amount of inference to perform.

==== Outputs

We can find our newly produced CrossCat model at `data/xcat/model.edn`.

=== Streaming Inference

==== Key points
* Experimental
* DVC yaml filename: `dvc-stream.yaml`

== References

TBD
