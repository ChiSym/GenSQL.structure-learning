= Automated Modeling
Ulli Schaechtle <u.schaechtle@gmail.com>

== Overview

This guide will help you use the IQL auto-modeling project to build your own
models on tabular data. Once built, these models can be queried using a variety
of interfaces. This guide starts with building a model from an example dataset.
It then shows you how to build a model from a dataset you supply. And latter
sections go into more detail about different backends for auto-modeling and a number of other considerations.

== Installation

=== Dependencies

We rely on the Nix package manager. Please find instructions to install it https://nixos.org/download.html[here]. Nix takes
care of all our dependencies. (On Windows, you will need to first https://docs.microsoft.com/en-us/windows/wsl/install[install WSL] and then install Nix within WSL's Ubuntu terminal, following the https://nixos.org/download.html#nix-install-windows[Nix installation instructions for WSL].)

=== Download Auto-modeling

If you are installing from a zipfile
[source,bash]
----
unzip InferenceQL.v0.zip
----
If you are installing from gity
[source,bash]
----
git clone git@github.com:OpenIQL/inferenceql.auto-modeling.git InferenceQL
----

=== Start the Nix shell

To install modeling capabilities, go to the auto-modeling directory:
[source,bash]
----
cd InferenceQL
----
and install the dependencies by running:
[source,bash]
----
nix-shell
----
WARNING: on macOS, this step can take up to an hour. The reason is that certain dependencies get compiled from scratch. It will only be slow once. The second time you run the `nix-shell` command, it will succeed instantaneously.

== Building your first model

=== Add a dataset

Next, we will add the dataset we want to model.

We are going to be working with a dataset from a citizen science project on
satellites:

[source,bash]
----
mkdir data
cp test/satellites/data.csv data/data.csv
----


=== Run the auto-modeling pipeline

IQL auto-modeling is built around DVC (https://dvc.org/), a project that helps with version control of machine learning pipelines.

The steps for building a model are encoded as dvc pipelines (see `dvc.yaml`) All we have to do is run the pipeline and dvc takes care of the rest.

From the root auto-modeling directory (`InferenceQL`), we can run the following command to get a list of the pipeline stages and which files they will produce giving us an overview of the work that will be done to build the model.
[source,bash]
----
dvc stage list
----

Now to run all of the pipelines executing
[source,bash]
----
dvc repro
----
Run `dvc repro --help` to see different options of running the pipeline.


== Quality control plots

Before we use our model, how can we know it is any good?

Auto-modeling produces two quality control apps for comparing simulated data produced by the model and the original observed data.

You can find them in `qc/app` folder.

`qc-dashboard.html` shows comparisons between all combinations of columns, while `qc-splom.html` shows a scatter plot matrix of just numerical columns -- the latter is not so interesting here as none of the numerical columns are inferred automatically (though there are several numerical columns in the data, which one can add manually as in `test/satellites/params.yaml`).

These apps are fully contained in their respective html files, so they can be easily moved around or shared.

The premise of the qc-plots is following: if our model has truly learned the multi-variate distribution of the data then the marginal and pairwise-marginal distributions of simulations from the model should match those of the observed data. These qc-plots allow one to visually confirm this.

NOTE: The plots support a number of features including panning, zooming, cross-selection, and details on hover. There are also other options in the options menu. You can find more details in the QC section of this guide.

== Querying the model
There are different ways to query automatically built models from Python,
Clojure and JavaScript. The easiest way to query modelsType the following and follow instructions:
[source,bash]
----
iql-query-server
----

== Configuring the pipeline

=== Data specifications/settings

There are a variety of settings in the `params.yaml` file in the auto-modeling root dir. The following might need to be changed depending on your dataset.

==== schema

Auto-modeling tries to guess the statistical data types in your CSV. You can see
which statistical types are guessed by running `dvc repro -f guess-schema` and then opening `data/schema.edn`.

If you want to manually set the data types for one or more columns you can do that in `schema` section in `params.yaml`.

TIP: If a statistical type cannot be guessed with confidence, auto-modeling chooses to ignore this column. You can add `default-stat-type: numerical` or `default-stat-type: nominal` if you prefer to define the stattypes of all columns that can't be guessed. This can save you some time when a lot of columns are ignored.

=== nullify

This setting allows you to specify which string values will be considered as
null values in your CSV. The system will treat strings as categories in a
categorical variable -- if you have, for example the string `NaN` encoding
missing data, you have to let the auto-modeling system know.

=== Inference-related settings

There are a number of settings in `params.yaml` file that allow you to control the inference process. See the section below on the CGPM backend for more details on these settings.

=== QC options

See the comments in the `qc` section of the `params.yaml` file for details on the various settings available for QC plots.


== Model-building backends

IQL Auto-modeling supports a number of model-building backends. The previous sections on model building used the default CGPM backend. We will provide some more background on the CGPM backend here and also provide information on using alternatives.

=== Switching between backends

Each backend is encoded as a `yaml` file. When `dvc repro -f` is run, the yaml file for backend currently named `dvc.yaml` is run. To switch to a different backend, rename `dvc.yaml` to any temporary name. And rename the yaml file for the backend to you want to use to `dvc.yaml`.

=== CGPM

==== Key points
* Default backend
* Written in Python
* Robust
* DVC yaml filename: `dvc.yaml`

==== Settings
The following settings in `params.yaml` allow you to control the inferece process using the default backend, CGPM.

- `sample_count` — This lets you set the number of CrossCat models to learn, which together will comprise the ensemble.
- `cgpm > minutes` — The amount of time (minutes) to spend on inference. Use this setting or `cgpm > iterations` but not both.
- `cgpm > iterations` — The number CGPM interations to spend on inference. Use this setting or `cgpm > minutes` but not both.

==== Outputs

The key artifacts produced are as follows.

===== Individual CrossCat models

In `data/xcat/`, you can find multiple CrossCat models. Each one is a `.edn` file named `sample.0.edn`, `sample.1.edn`, etc. Any one of these individual CrossCat models can be used in an Observable notebook or in the IQL Viz spreadsheet app.

===== Ensemble of CrossCat models

`data/sppl/merged.json` is a sum-product network representation of all of the individual CrossCat models merged together forming an ensemble. This file can be used by IQL Query to start an IQL query server. The query server can then respond to sum-product queries from both an Observable notebook and the IQL Viz spreadsheet app. This is covered in a latter section.

=== Loom and CGPM

==== Key points
* Loom used to learn structure
* CGPM used to learn hyperparameters
* Loom is written in C with Python bindings
* Robust
* DVC yaml filename: `dvc-loom.yaml`

==== Setup
TODO: notes on getting the Docker image.

==== Settings
All the settings in `params.yaml` that apply to the CGPM backend also apply to the LOOM + CGPM backend. In addition, there are the following.

- `loom > extra_passes` — The number of extra inference passes to perform when learning structure.

==== Outputs
The outputs produced are the same as those produced by the CGPM backend. Please see the ouputs section for that backend.

=== Clojurecat

==== Key points
* Written in Clojure
* Usable from both the JVM and the browser (JS environments)
* Fewest requirements
* Experimental (there are know issues)
* DVC yaml filename: `dvc-clojurecat.yaml`

==== Settings
- `clojurecat > iterations` — This setting controls the amount of inference to perform.

==== Outputs

We can find our newly produced CrossCat model at `data/xcat/model.edn`.

=== Streaming Inference

==== Key points
* Experimental
* DVC yaml filename: `dvc-stream.yaml`

== References

TBD

== Publish your results

The auto-modeling pipeline prepares a directory for users to publish models and populations and supply the GitHub CLI tool to make it easy for users to contribute their results to a repository collecting analysis output.
[source,bash]
----
cd publish-analysis-output && gh auth login -w -h GitHub.com
----
This command will ask you to authenticate yourself with GitHub.
[source,bash]
----
gh pr create --base public --title "My analysis on [add dataset name and analysis goal here]" --body "Add more info here"
----
This command will fork our analyses repository, push your data and results to a branch, and open a pull request back to our repo. Users can use git to customize what get's published.
