stages:
  validate-column-names:
    cmd: bin/validate.sh
    deps:
      - data/data.csv
    outs:
      - data/validated.csv

  nullify:
    cmd: >
      clojure -X gensql.structure-learning.main/nullify
      < data/validated.csv
      > data/nullified.csv
    deps:
      - data/validated.csv
    params:
      - nullify
    outs:
      - data/nullified.csv

  guess-schema:
    cmd: >
      clojure -X gensql.structure-learning.main/guess-schema
      < data/nullified.csv
      > data/schema.edn
    deps:
      - data/nullified.csv
    params:
      - schema
    outs:
      - data/schema.edn

  cgpm-schema:
    cmd: >
      clojure -X gensql.structure-learning.main/cgpm-schema
      < data/schema.edn
      > data/cgpm-schema.edn
    deps:
      - data/schema.edn
    outs:
      - data/cgpm-schema.edn

  ignore:
    cmd: >
      clojure -X gensql.structure-learning.main/ignore
      :schema '"data/schema.edn"'
      < data/nullified.csv
      > data/ignored.csv
    deps:
      - data/nullified.csv
      - data/schema.edn
    outs:
      - data/ignored.csv

  numericalize:
    cmd: >
      clojure -X gensql.structure-learning.main/numericalize
      :schema '"data/schema.edn"'
      :table data/mapping-table.edn
      < data/ignored.csv
      > data/numericalized.csv
    deps:
      - data/ignored.csv
      - data/schema.edn
    outs:
      - data/numericalized.csv
      - data/mapping-table.edn

  cgpm-infer-stream:
    desc: >
      This produces both complete cgpm-model exports after inference is complete and
      cgpm-model checkpoints while inference is progressing.
    cmd:
      - mkdir -p data/cgpm/complete
      - >-
        parallel ${parallel.flags}
        'mkdir -p data/cgpm/checkpoints/sample-{}/'
        :::: <(seq ${seed} $((${seed} + ${sample_count} - 1 )))
      - >-
        parallel ${parallel.flags}
        'python scripts/cgpm_stream.py
        --output data/cgpm/complete/sample.{}.json
        --data data/numericalized.csv
        --schema data/cgpm-schema.edn
        --mapping-table data/mapping-table.edn
        --seed {}'
        :::: <(seq ${seed} $((${seed} + ${sample_count} - 1 )))
    params:
      - parallel.flags
      - sample_count
      - seed
    deps:
      - data/cgpm-schema.edn
      - data/mapping-table.edn
      - data/numericalized.csv
      - schemas/cgpm.json
      - scripts/cgpm_stream.py
      - scripts/inf_prog.py
    outs:
      - data/cgpm/complete
      - data/cgpm/checkpoints

  summarize-checkpoints:
    desc: >
      This takes the cgpm-model checkpoints for each crosscat sample and concatenates them into a
      single array in a json file.
    cmd:
      - mkdir -p data/cgpm/transitions
      - >-
        parallel ${parallel.flags}
        'jq -s "." {1}/t-*.json > data/cgpm/transitions/{/}.json'
        :::: <(find data/cgpm/checkpoints/sample-* -type d)
    deps:
      - data/cgpm/checkpoints
    outs:
      - data/cgpm/transitions

  ### Stages related to dependencies between columns (dep-prob).

  cgpm-dep-prob-transitions:
    desc: >
      This produces the dependency probability between columns for each cgpm-model checkpoint.
    cmd:
      - >-
        parallel mkdir -p data/cgpm/dep-prob/checkpoints/{}
        :::: <(find data/cgpm/checkpoints/sample-* -type d | cut -d/ -f4)
      - >-
        parallel 'python scripts/dep_prob.py
        --data data/numericalized.csv
        --output data/cgpm/dep-prob/checkpoints/{2} {1}'
        :::: <(find data/cgpm/checkpoints -type f)
        ::::+ <(find data/cgpm/checkpoints -type f | cut -d/ -f4-5)
    params:
      - parallel.flags
    deps:
      - data/numericalized.csv
      - data/cgpm/checkpoints
    outs:
      - data/cgpm/dep-prob/checkpoints/

  cgpm-dep-prob-merge:
    desc: >
      This concatenates the dependency probabilities produced from many
      cgpm-model checkpoints into a json arrays (one for each crosscat sample).
    cmd:
      - mkdir -p data/cgpm/dep-prob/transitions/
      - >-
        parallel "jq --slurp '.' {1}/t-*.json > data/cgpm/dep-prob/transitions/{2}.json"
        :::: <(find data/cgpm/dep-prob/checkpoints/sample-* -type d)
        ::::+ <(find data/cgpm/dep-prob/checkpoints/sample-* -type d | cut -d/ -f5)
    deps:
      - data/cgpm/dep-prob/checkpoints/
    outs:
      - data/cgpm/dep-prob/transitions/

  cgpm-dep-prob-merge-phase-2:
    desc: >
      This concatenates the dependency probabilities produced from each crosscat sample into a
      json array.
    cmd: >
      jq --slurp '.' data/cgpm/dep-prob/transitions/sample-*.json
      >  data/cgpm/dep-prob/transitions-merged-not-reduced.json
    deps:
      - data/cgpm/dep-prob/transitions/
    outs:
      - data/cgpm/dep-prob/transitions-merged-not-reduced.json

  cgpm-dep-prob-merge-phase-3:
    desc: >
      This reduces the binary dep-prob values in the previous stage into values ranging [0,1] by
      averaging across all crosscat models at each iteration (cgpm-model checkpoint).
    cmd: >-
      clojure -X gensql.structure-learning.stream.dep-prob/reduce
      :dep-prob data/cgpm/dep-prob/transitions-merged-not-reduced.json
      > data/cgpm/dep-prob/transitions-merged.json
    deps:
      - data/cgpm/dep-prob/transitions-merged-not-reduced.json
    outs:
      - data/cgpm/dep-prob/transitions-merged.json

  ### Stages related to converting CGPM models to XCat records.

  xcat-transitions-import:
    desc: >
      Converts the sequences of cgpm-model exports found in cgpm transitions files into
      sequences of xcat-models (exported as transit-encoded strings).
    cmd:
      - mkdir -p data/xcat/transitions
      - >-
        parallel ${parallel.flags}
        'clojure -X gensql.structure-learning.stream.xcat/transitions-import
        :transitions-path {1}
        :data-csv data/ignored.csv
        :mapping-table data/mapping-table.edn
        :numericalized-csv data/numericalized.csv
        :schema-edn data/schema.edn
        > data/xcat/transitions/{2}'
        :::: <(find data/cgpm/transitions -type f | sort)
        ::::+ <(find data/cgpm/transitions -type f | sort | cut -d/ -f4)
    deps:
      - data/cgpm/transitions
      - data/mapping-table.edn
      - data/ignored.csv
      - data/numericalized.csv
      - data/schema.edn
      - src/gensql/structure_learning/xcat.clj
    outs:
      - data/xcat/transitions

  xcat-transitions-merge:
    desc: >
      Produces a sequences of ensembles which consist of a sequence of xcat-models (exported
      as transit-encoded strings). This is done by merging the models at each iteration in the
      xcat-transitions files.

      NOTE: The :transitions-paths argument requires special quoting. First we are passing double
      quotes to the Clojure via the single-quoted double-quotes [e.g. '"'] on both ends of the
      argument. This is so the inner contents gets treated as a string by Clojure. Then we are
      double-quoting the bash command, [e.g. "$(find...)"] so that shell does not try to evaluate
      the paths that are produced from the command. Instead it will be passed as text to Clojure.
    cmd:
      - >-
        clojure -X gensql.structure-learning.stream.xcat/transitions-merge
        :transitions-paths '"'"$(find data/xcat/transitions -type f | sort)"'"'
        > data/xcat/transitions-merged.json
    deps:
      - data/xcat/transitions
    outs:
      - data/xcat/transitions-merged.json

  xcat-transitions-sample:
    desc: >
      Samples 1000 points from the ensemble of xcat models at each iteration. Each sample is
      produced using a model chosen randomly from the ensemble. The collection of samples produced
      is a transit-encoded string of a clojure collection.
    cmd: >-
      clojure -X gensql.structure-learning.stream.xcat/transitions-sample
      :schema-edn data/schema.edn
      :transitions-merged data/xcat/transitions-merged.json
      > data/xcat/transitions-samples.json
    params:
      - params-stream.yaml:
          - allow_negative_simulations
    deps:
      - data/schema.edn
      - data/xcat/transitions-merged.json
    outs:
      - data/xcat/transitions-samples.json

  xcat-transitions-sample-compress:
    desc: >
      Compresses the transit-encoded string of samples using lz-string.
    cmd:
      - >-
        node lz-compress.js data/xcat/transitions-samples.json
        > data/xcat/transitions-samples.lzstring.json
    deps:
      - data/xcat/transitions-samples.json
    outs:
      - data/xcat/transitions-samples.lzstring.json

  ### Stages related to converting XCat reconds to Javascript programs.

  mmix-transitions-import:
    desc: >
      Converts the sequences of xcat-model checkpoints found in transitions files into sequences
      mmix specs. Each transitions file will become a transit-encoded string of a clojure collection
      of mmix-specs.
    cmd:
      - mkdir -p data/mmix/transitions
      - >-
        parallel ${parallel.flags}
        'clojure -X gensql.structure-learning.stream.mmix/transitions-import
        :transitions-path {1}
        > data/mmix/transitions/{2}'
        :::: <(find data/xcat/transitions -type f | sort)
        ::::+ <(find data/xcat/transitions -type f | sort | cut -d/ -f4)
    deps:
      - data/xcat/transitions
    outs:
      - data/mmix/transitions

  jsprogram-transitions-import:
    desc: >
      Produces a sequences of javascript programs (strings) representing the model encoded by
      multimix specs. A sequence is produced for each crosscat sample.
    cmd:
      - mkdir -p data/jsprogram/transitions
      - >-
        parallel ${parallel.flags}
        'clojure -X gensql.structure-learning.stream.jsmodel.import/transitions-import
        :transitions-path {1}
        > data/jsprogram/transitions/{2}'
        :::: <(find data/mmix/transitions -type f | sort)
        ::::+ <(find data/mmix/transitions -type f | sort | cut -d/ -f4)
    deps:
      - data/mmix/transitions
    outs:
      - data/jsprogram/transitions

  jsprogram-transitions-merge:
    desc: >
      This produces ensembles of javascript programs (strings) by merging the programs found at
      each iteration in the sequences of programs produced for each crosscat sample.
    cmd:
      - >-
        clojure -X gensql.structure-learning.stream.jsmodel.import/transitions-merge
        :transitions-paths '"'"$(find data/jsprogram/transitions -type f | sort)"'"'
        > data/jsprogram/transitions-merged.json
    deps:
      - data/jsprogram/transitions
    outs:
      - data/jsprogram/transitions-merged.json

  ### Stages related to building the iql.viz.stream.
  
  iql.viz.stream-source:
    desc: "Checks out the source tree for the iql.viz.stream app."
    wdir: learning-dashboard/
    cmd:
      - rm -rf source
      - mkdir source
      - >-
        git clone -b main --single-branch --depth 1
        git@github.com:probcomp/inferenceql.viz.stream.git source/
    outs:
      - source/index.html
      - source/resources/config-template.edn

  iql.viz.stream-transitions-shorten-ensemble:
    desc: >
      Shortens the size of the ensembles in the sequence of ensembles. Each ensemble is shortened
      to only have 3 xcat models (transit-encoded strings). The new sequences of ensembles is copied
      over as a temp-resource for iql.viz.stream.
    cmd:
      - >-
        clojure -X gensql.structure-learning.stream.xcat/transitions-shorten-ensemble
        :transitions-merged data/xcat/transitions-merged.json
        > learning-dashboard/source/temp-resources/transitions.json
    deps:
      - learning-dashboard/source/index.html # Ensures learning-dashboard-source tree is available.
      - data/xcat/transitions-merged.json
    outs:
      - learning-dashboard/source/temp-resources/transitions.json

  iql.viz.stream-config:
    desc: "Creates a config.edn file for the iql.viz.stream app."
    cmd:
      - >-
        clojure -X gensql.structure-learning.stream.config/generate
        :config learning-dashboard/source/resources/config-template.edn
        :mapping-table data/mapping-table.edn
        :transitions-merged data/xcat/transitions-merged.json
        > learning-dashboard/source/resources/config.edn
    params:
      - params-stream.yaml:
          - slider_text
          - allow_negative_simulations
          - numerical_ranges
    deps:
      - learning-dashboard/source/resources/config-template.edn
      - data/xcat/transitions-merged.json
    outs:
      - learning-dashboard/source/resources/config.edn

  iql.viz.stream-resources:
    desc: "Copies various resources and temp-resources to the iql.viz.stream app."
    cmd:
      - cp data/ignored.csv learning-dashboard/source/resources/
      - cp data/schema.edn learning-dashboard/source/resources/
      - >-
        cp data/xcat/transitions-samples.lzstring.json
        learning-dashboard/source/temp-resources/transitions-samples.json
      - >-
        cp data/cgpm/dep-prob/transitions-merged.json
        learning-dashboard/source/temp-resources/mutual-info.json
      - >-
        cp data/jsprogram/transitions-merged.json
        learning-dashboard/source/temp-resources/js-program-transitions.json
    deps:
      - data/ignored.csv
      - data/schema.edn
      - data/xcat/transitions-merged.json
      - data/xcat/transitions-samples.lzstring.json
      - data/jsprogram/transitions-merged.json
      - data/cgpm/dep-prob/transitions-merged.json
      - learning-dashboard/source/temp-resources/transitions.json
      - learning-dashboard/source/index.html
    outs:
      - learning-dashboard/source/resources/ignored.csv
      - learning-dashboard/source/resources/schema.edn
      - learning-dashboard/source/temp-resources/transitions-samples.json
      - learning-dashboard/source/temp-resources/js-program-transitions.json
      - learning-dashboard/source/temp-resources/mutual-info.json

  iql.viz.stream-compile:
    desc: "Compiles the iql.viz.stream app into a standalone JS app."
    wdir: learning-dashboard/source/
    cmd: make js-advanced-min
    deps:
      - resources/config.edn
      - resources/ignored.csv
      - resources/schema.edn
      - temp-resources/transitions-samples.json
      - temp-resources/js-program-transitions.json
      - temp-resources/mutual-info.json
      - temp-resources/transitions.json
    outs:
      - out/

  iql.viz.stream-package:
    desc: "Gathers all the files needed run the compiled iql.viz.stream app into a folder."
    wdir: learning-dashboard/
    cmd:
      - rm -rf dist
      - mkdir dist
      - cp source/index.html dist
      - cp -r source/resources dist
      - cp -r source/out dist
    deps:
      - source/out/
    outs:
      - dist/index.html
      - dist/resources
      - dist/out

  iql.viz.stream-zip:
    desc: "Compresses the files needed to run the compiled iql.viz.stream app."
    wdir: learning-dashboard/
    cmd:
      - zip -rq dist.zip dist
    deps:
      - dist/index.html
      - dist/resources
      - dist/out
    outs:
      - dist.zip

  iql.viz.stream-inline:
    desc: >
      Inlines all the iql.viz.stream files into a single html file.
      NOTE: This stage may fail when the resources for iql.viz.stream are too large.
    wdir: learning-dashboard/
    cmd:
      - inliner -n source/index.html > dist-inlined.html 2> /dev/null
    deps:
      - source/out/
    outs:
      - dist-inlined.html
